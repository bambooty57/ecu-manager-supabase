import { supabase, uploadFileToStorage, getBucketForFileType, generateUniqueFileName } from './supabase'
import { Tables } from './database.types'

type WorkRecordData = Tables<'work_records'>

const BUCKET_CONFIG: { [key: string]: { name: string; extensions: string[] } } = {
  media: {
    name: 'work-media',
    extensions: ['jpg', 'jpeg', 'png', 'gif', 'webp', 'avif', 'mp4', 'avi', 'mov', 'wmv', 'flv'],
  },
  docs: {
    name: 'work-documents',
    extensions: ['pdf', 'doc', 'docx', 'xls', 'xlsx', 'ppt', 'pptx', 'txt', 'zip', 'mmf'],
  },
  misc: {
    name: 'work-files',
    extensions: [], // for any other type
  }
};

// ë§ˆì´ê·¸ë ˆì´ì…˜ìš© ì‘ì—… ê¸°ë¡ ì¡°íšŒ (ëª¨ë“  ë°ì´í„° í¬í•¨)
const getAllWorkRecordsForMigration = async (): Promise<WorkRecordData[]> => {
  const { data, error } = await supabase
    .from('work_records')
    .select('*')
    .order('created_at', { ascending: false })
  
  if (error) {
    console.error('Error fetching work records for migration:', error)
    throw error
  }
  
  return data
}

// Base64ë¥¼ íŒŒì¼ ê°ì²´ë¡œ ë³€í™˜
export const base64ToFile = (base64: string, filename: string, mimeType: string): File => {
  console.log(`  [base64ToFile] Converting for "${filename}"`);
  console.log(`  [base64ToFile] Input data (first 80 chars): ${base64.substring(0, 80)}...`);

  let cleanBase64 = base64;
  const match = base64.match(/^data:.*?;base64,(.*)$/);
  if (match) {
    console.log("  [base64ToFile] Data URL format detected. Extracting base64 content.");
    cleanBase64 = match[1];
  }

  // Handle non-standard base64 characters and padding
  console.log("  [base64ToFile] Cleaning base64 string for atob compatibility...");
  cleanBase64 = cleanBase64.replace(/-/g, '+').replace(/_/g, '/');
  cleanBase64 = cleanBase64.replace(/\s/g, '');
  const padding = '='.repeat((4 - cleanBase64.length % 4) % 4);
  cleanBase64 += padding;

  try {
    const byteCharacters = atob(cleanBase64);
    const byteNumbers = new Array(byteCharacters.length);
    for (let i = 0; i < byteCharacters.length; i++) {
      byteNumbers[i] = byteCharacters.charCodeAt(i);
    }
    const byteArray = new Uint8Array(byteNumbers);
    const blob = new Blob([byteArray], { type: mimeType });
    console.log(`  [base64ToFile] Created blob with size: ${blob.size} for file: ${filename}`);
    return new File([blob], filename, { type: mimeType });
  } catch (e: any) {
    console.error(`  [base64ToFile] ğŸ”´ Failed to decode base64 for "${filename}". Error: ${e.message}`);
    console.error(`  [base64ToFile] Faulty data (first 80 chars): ${cleanBase64.substring(0, 80)}...`);
    throw e; // re-throw the error to be caught by the calling function
  }
};

// ë‹¨ì¼ íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜
const migrateSingleFile = async (
  base64Data: string,
  fileName: string,
  mimeType: string,
  workRecordId: number | string,
  category: string
): Promise<any> => {
  console.log(`  [migrateSingleFile] Migrating "${fileName}" for record ${workRecordId}`);
  const file = base64ToFile(base64Data, fileName, mimeType);
  const bucket = getBucketForFileType(fileName);
  const uniqueName = generateUniqueFileName(fileName);
  const storagePath = `${workRecordId}/${uniqueName}`;

  // 1. Upload to Storage
  console.log(`  [migrateSingleFile] Uploading to bucket: "${bucket}", path: "${storagePath}"`);
  const { data: uploadData, error } = await supabase.storage
    .from(bucket)
    .upload(storagePath, file);

  if (error) {
    console.error(`  [migrateSingleFile] ğŸ”´ Storage upload failed for "${fileName}":`, error);
    throw new Error(`Storage ì—…ë¡œë“œ ì‹¤íŒ¨: ${error.message}`);
  }

  // 2. Get public URL
  const { data: urlData } = supabase.storage.from(bucket).getPublicUrl(storagePath);
  
  // 3. Insert metadata
  const metadata = {
    work_record_id: typeof workRecordId === 'string' ? parseInt(workRecordId, 10) : workRecordId,
    file_name: uniqueName,
    original_name: fileName,
    file_size: file.size,
    file_type: file.type,
    category: category,
    bucket_name: bucket,
    storage_path: uploadData.path,
    storage_url: urlData.publicUrl,
  };

  const { data: insertData, error: insertError } = await supabase
    .from('file_metadata')
    .insert(metadata)
    .select()
    .single();

  if (insertError) {
    console.error(`  [migrateSingleFile] ğŸ”´ Metadata insert failed for "${fileName}":`, insertError);
    // TODO: Consider deleting the uploaded file from storage to avoid orphans
    throw new Error(`ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: ${insertError.message}`);
  }

  console.log(`  [migrateSingleFile] âœ… Successfully migrated "${fileName}"`);
  return insertData;
};

// ë‹¨ì¼ íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜
export const migrateFileToStorage = async (
  fileData: any,
  workRecordId: number,
  category: string
): Promise<{ storagePath: string, storageUrl: string, bucketName: string } | null> => {
  try {
    console.log(`ğŸ” íŒŒì¼ ë°ì´í„° êµ¬ì¡° í™•ì¸:`, {
      hasData: !!fileData.data,
      hasName: !!fileData.name,
      hasType: !!fileData.type,
      dataType: typeof fileData.data,
      dataLength: fileData.data?.length || 0,
      fileName: fileData.name,
      fileType: fileData.type
    })
    
    if (!fileData.data || !fileData.name) {
      console.log('âŒ íŒŒì¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤:', fileData)
      return null
    }

    // File ê°ì²´ ìƒì„±
    const file = base64ToFile(fileData.data, fileData.name, fileData.type || 'application/octet-stream')
    
    // ë²„í‚· ë° íŒŒì¼ëª… ê²°ì •
    const bucketName = getBucketForFileType(file.name)
    const uniqueFileName = generateUniqueFileName(file.name)
    
    // Storageì— ì—…ë¡œë“œ
    const uploadResult = await uploadFileToStorage(file, bucketName, uniqueFileName)
    
    // íŒŒì¼ ë©”íƒ€ë°ì´í„° DB ì €ì¥
    const { error: metadataError } = await supabase
      .from('file_metadata')
      .insert({
        work_record_id: workRecordId,
        file_name: file.name,
        file_size: file.size,
        file_type: file.type,
        category: category,
        storage_path: uploadResult.path,
        storage_url: uploadResult.url,
        bucket_name: bucketName,
        description: fileData.description || ''
      })

    if (metadataError) {
      console.error('íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥ ì˜¤ë¥˜:', metadataError)
      throw metadataError
    }

    console.log(`âœ… íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: ${file.name} â†’ ${bucketName}/${uniqueFileName}`)
    
    return {
      storagePath: uploadResult.path,
      storageUrl: uploadResult.url,
      bucketName: bucketName
    }
  } catch (error) {
    console.error('íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ ì˜¤ë¥˜:', error)
    return null
  }
}

// ì‘ì—… ê¸°ë¡ì˜ ëª¨ë“  íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜
export const migrateWorkRecordFiles = async (workRecord: WorkRecordData): Promise<boolean> => {
  try {
    console.log(`ğŸ”„ ì‘ì—… ê¸°ë¡ ${workRecord.id} íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘...`)
    
    // remapping_worksëŠ” Json íƒ€ì…ì´ë¯€ë¡œ ë°°ì—´ë¡œ íŒŒì‹±
    const remappingWorks = Array.isArray(workRecord.remapping_works) 
      ? workRecord.remapping_works 
      : (workRecord.remapping_works ? [workRecord.remapping_works] : [])
    
    if (!remappingWorks || remappingWorks.length === 0) {
      console.log('ë§ˆì´ê·¸ë ˆì´ì…˜í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.')
      return true
    }

    const firstWork = remappingWorks[0] as any
    let migratedFiles: any[] = []
    let migrationCount = 0

    // ECU íŒŒì¼ë“¤ ë§ˆì´ê·¸ë ˆì´ì…˜
    if (firstWork.files) {
      const ecuCategories = ['original', 'read', 'modified', 'vr', 'stage1', 'stage2', 'stage3']
      
      for (const category of ecuCategories) {
        const fileData = (firstWork.files as any)[category]
        if (fileData && fileData.file) {
          const result = await migrateFileToStorage(fileData.file, workRecord.id, `ecu_${category}`)
          if (result) {
            migratedFiles.push({
              category: `ecu_${category}`,
              originalData: fileData,
              storageInfo: result
            })
            migrationCount++
          }
        }
      }
    }

    // ACU íŒŒì¼ë“¤ ë§ˆì´ê·¸ë ˆì´ì…˜
    if (firstWork.acu && firstWork.acu.files) {
      const acuCategories = ['acuOriginal', 'acuStage1', 'acuStage2', 'acuStage3']
      
      for (const category of acuCategories) {
        const fileData = (firstWork.acu.files as any)[category]
        if (fileData && fileData.file) {
          const result = await migrateFileToStorage(fileData.file, workRecord.id, category)
          if (result) {
            migratedFiles.push({
              category: category,
              originalData: fileData,
              storageInfo: result
            })
            migrationCount++
          }
        }
      }
    }

    // ë¯¸ë””ì–´ íŒŒì¼ë“¤ ë§ˆì´ê·¸ë ˆì´ì…˜
    if (firstWork.media) {
      const mediaCategories = ['before', 'after']
      
      for (const category of mediaCategories) {
        const mediaData = (firstWork.media as any)[category]
        if (mediaData) {
          const result = await migrateFileToStorage(mediaData, workRecord.id, `media_${category}`)
          if (result) {
            migratedFiles.push({
              category: `media_${category}`,
              originalData: mediaData,
              storageInfo: result
            })
            migrationCount++
          }
        }
      }
    }

    // ì¶”ê°€ ë¯¸ë””ì–´ íŒŒì¼ë“¤ (mediaFile1~5)
    if (firstWork.files) {
      for (let i = 1; i <= 5; i++) {
        const mediaFile = (firstWork.files as any)[`mediaFile${i}`]
        if (mediaFile && mediaFile.file) {
          const result = await migrateFileToStorage(mediaFile.file, workRecord.id, `media_file_${i}`)
          if (result) {
            migratedFiles.push({
              category: `media_file_${i}`,
              originalData: mediaFile,
              storageInfo: result
            })
            migrationCount++
          }
        }
      }
    }

    console.log(`âœ… ì‘ì—… ê¸°ë¡ ${workRecord.id} ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: ${migrationCount}ê°œ íŒŒì¼`)
    return true

  } catch (error) {
    console.error(`âŒ ì‘ì—… ê¸°ë¡ ${workRecord.id} ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨:`, error)
    return false
  }
}

// ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ë©”ì¸ í•¨ìˆ˜
export const migrateAllFilesToStorage = async (
  onProgress: (current: number, total: number, recordId: number | string) => void
): Promise<{ success: number; failed: number }> => {
  const stats = { succeeded: 0, failed: 0, skipped: 0, totalFiles: 0 };
  console.log('ğŸš€ [MIGRATION START] ì „ì²´ íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.');

  const recordsToProcess = await getAllWorkRecordsForMigration();

  if (recordsToProcess.length === 0) {
    console.log('ğŸ [MIGRATION END] ì²˜ë¦¬í•  ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.');
    return { success: 0, failed: 0 };
  }

  for (const [index, record] of recordsToProcess.entries()) {
    onProgress(index + 1, recordsToProcess.length, record.id);
    console.log(`\n--- [Processing Record ${index + 1}/${recordsToProcess.length}] ID: ${record.id} ---`);

    // Check if already migrated by looking at file_metadata table
    const { count: existingFiles } = await supabase
      .from('file_metadata')
      .select('*', { count: 'exact', head: true })
      .eq('work_record_id', record.id);
    
    if (existingFiles && existingFiles > 0) {
      console.log(`  > ğŸŸ¢ [ALREADY MIGRATED] ID ${record.id}ëŠ” ì´ë¯¸ ë§ˆì´ê·¸ë ˆì´ì…˜ë˜ì—ˆìŠµë‹ˆë‹¤. (${existingFiles}ê°œ íŒŒì¼)`);
      stats.skipped++;
      continue;
    }
    
    console.log(`  [step 2.1] ID ${record.id}ì˜ íŒŒì¼ ë°ì´í„° ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤.`);
    let filesToMigrate: any[] = [];
    let rawFilesData: any = null;

    const recordFromDb = record as any;
    const potentialFields = ['remappingWorks', 'remapping_works', 'files'];

    for (const field of potentialFields) {
      console.log(`    - ${field} í•„ë“œì—ì„œ ë°ì´í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.`);
      const data = recordFromDb[field];
      
      if (data) {
        let parsedData: any = null;
        if (typeof data === 'string') {
          console.log(`      - ${field}ê°€ ë¬¸ìì—´ì…ë‹ˆë‹¤. íŒŒì‹±ì„ ì‹œë„í•©ë‹ˆë‹¤.`);
          try {
            parsedData = JSON.parse(data);
          } catch (e) {
            console.error(`      - ğŸ”´ ${field} 1ì°¨ JSON íŒŒì‹± ì‹¤íŒ¨:`, e);
            parsedData = null;
          }
        } else {
          parsedData = data;
        }

        // Handle double-stringified JSON
        if (typeof parsedData === 'string') {
            console.log(`      - ë°ì´í„°ê°€ ì´ì¤‘ìœ¼ë¡œ ë¬¸ìì—´í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ íŒŒì‹±ì„ ì‹œë„í•©ë‹ˆë‹¤.`);
            try {
                rawFilesData = JSON.parse(parsedData);
            } catch (e) {
                console.error(`      - ğŸ”´ ${field} 2ì°¨ JSON íŒŒì‹± ì‹¤íŒ¨:`, e);
                rawFilesData = null;
            }
        } else {
            rawFilesData = parsedData;
        }

        if (rawFilesData && Array.isArray(rawFilesData)) {
            filesToMigrate = rawFilesData.filter(file => file && (file.file_base64 || file.base64 || file.data));
            if (filesToMigrate.length > 0) {
              console.log(`      - âœ… ${field}ì—ì„œ ${filesToMigrate.length}ê°œì˜ ìœ íš¨í•œ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.`);
              break; 
            }
        }
      }
    }

    if (filesToMigrate.length === 0) {
      console.log('    - ğŸŸ¡ ë§ˆì´ê·¸ë ˆì´ì…˜í•  íŒŒì¼ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.');
      const debugData = {
        id: record.id,
        created_at: record.created_at,
        remappingWorks: recordFromDb.remappingWorks ? 'Found' : 'Not Found',
        remapping_works: recordFromDb.remapping_works ? 'Found' : 'Not Found',
        files: recordFromDb.files ? 'Found' : 'Not Found',
      }
      console.log('    - [DEBUG] Record relevant fields status:', JSON.stringify(debugData, null, 2));
    }

    // If no files found, skip
    if (filesToMigrate.length === 0) {
      console.log(`  > ğŸŸ¡ [SKIPPED] ID ${record.id} ë§ˆì´ê·¸ë ˆì´ì…˜ ëŒ€ìƒ íŒŒì¼ ì—†ìŒ.`);
      stats.skipped++;
      continue;
    }

    console.log(`  [step 2.2] ID ${record.id}ì— ëŒ€í•´ ì´ ${filesToMigrate.length}ê°œì˜ íŒŒì¼ì„ ë§ˆì´ê·¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.`);
    stats.totalFiles += filesToMigrate.length;

    let successInRecord = true;
    let migratedCountInRecord = 0;
    for (const [fileIndex, file] of filesToMigrate.entries()) {
      try {
        // Adapt to possible variations in property names (file_base64 or base64 or data)
        const base64Data = file.file_base64 || file.base64 || file.data;
        const fileName = file.file_name || file.name || 'unknown_file';

        if (!base64Data) {
          console.log(`      - ğŸŸ¡ íŒŒì¼ì— Base64 ë°ì´í„°ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤: ${fileName}`);
          continue;
        }
        
        console.log(`    [step 2.2.${fileIndex + 1}] "${fileName}" íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘...`);

        const newFileMetadata = await migrateSingleFile(
          base64Data,
          fileName,
          file.type || 'application/octet-stream',
          record.id,
          file.category || 'unknown'
        );

        if (newFileMetadata) {
          migratedCountInRecord++;
        } else {
          successInRecord = false;
        }
      } catch (e: any) {
        successInRecord = false;
        console.error(`  > ğŸ”´ [ERROR] íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (File: ${file.file_name || file.name}, Record ID: ${record.id}):`, e);
      }
    }

    // After processing all files for a record, update stats
    if (successInRecord) {
      console.log(`  > âœ… [SUCCESS] ID ${record.id}ì˜ ëª¨ë“  íŒŒì¼ (${migratedCountInRecord}ê°œ) ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ê³µ.`);
      stats.succeeded++;
    } else {
      console.log(`  > ğŸ”´ [FAILED] ID ${record.id}ì˜ íŒŒì¼ ì¤‘ ì¼ë¶€ ë˜ëŠ” ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨.`);
      stats.failed++;
    }
  }

  console.log('\nğŸ [MIGRATION END] ë§ˆì´ê·¸ë ˆì´ì…˜ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.');
  console.log(`   - ì„±ê³µ: ${stats.succeeded}ê±´`);
  console.log(`   - ì‹¤íŒ¨: ${stats.failed}ê±´`);
  console.log(`   - ìŠ¤í‚µ: ${stats.skipped}ê±´`);
  console.log(`   - ì´ ì²˜ë¦¬ëœ íŒŒì¼: ${stats.totalFiles}ê°œ`);

  return { success: stats.succeeded, failed: stats.failed };
}

// ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒíƒœ í™•ì¸ (ëŒ€ì‹œë³´ë“œìš©)
export const checkMigrationStatus = async () => {
  try {
    console.log('ğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒíƒœ í™•ì¸ ì¤‘...');
    
    // 1. ì „ì²´ ì‘ì—… ê¸°ë¡ ìˆ˜
    const { count: totalRecords, error: totalError } = await supabase
      .from('work_records')
      .select('*', { count: 'exact', head: true });

    if (totalError) {
      console.error('âŒ ì „ì²´ ì‘ì—… ê¸°ë¡ ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨:', totalError);
      throw totalError;
    }

    // 2. ë§ˆì´ê·¸ë ˆì´ì…˜ëœ íŒŒì¼ ìˆ˜
    const { count: migratedFiles, error: migratedError } = await supabase
      .from('file_metadata')
      .select('*', { count: 'exact', head: true });

    if (migratedError) {
      console.error('âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ëœ íŒŒì¼ ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨:', migratedError);
      throw migratedError;
    }

    // 3. ë§ˆì´ê·¸ë ˆì´ì…˜ëœ ë ˆì½”ë“œ ìˆ˜ (ê³ ìœ í•œ work_record_id)
    const { data: distinctRecords, error: distinctError } = await supabase
      .from('file_metadata')
      .select('work_record_id')
      .not('work_record_id', 'is', null);

    if (distinctError) {
      console.error('âŒ ê³ ìœ  ë ˆì½”ë“œ ì¡°íšŒ ì‹¤íŒ¨:', distinctError);
      throw distinctError;
    }

    const uniqueRecordIds = new Set(distinctRecords?.map(r => r.work_record_id) || []);
    const migratedRecords = uniqueRecordIds.size;

    console.log('âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒíƒœ ì¡°íšŒ ì™„ë£Œ:', {
      totalRecords,
      migratedFiles,
      migratedRecords
    });

    return {
      totalRecords: totalRecords || 0,
      migratedFiles: migratedFiles || 0,
      migratedRecords,
      migrationProgress: totalRecords ? Math.round((migratedRecords / totalRecords) * 100) : 0
    };
  } catch (error) {
    console.error('âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨:', error);
    throw error;
  }
};

// ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒíƒœ í™•ì¸
export const getMigrationStatus = async (): Promise<{ total: number; migrated: number }> => {
  const { count: total, error: totalError } = await supabase
    .from('work_records')
    .select('*', { count: 'exact', head: true });

  if (totalError) {
    console.error('ğŸ”´ ì „ì²´ ì‘ì—… ê¸°ë¡ ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨:', totalError);
    return { total: 0, migrated: 0 };
  }

  // Count distinct work_record_id from file_metadata table
  const { data: migratedRecords, error: migratedError } = await supabase
    .from('file_metadata')
    .select('work_record_id')
    .not('work_record_id', 'is', null);

  if (migratedError) {
    console.error('ğŸ”´ ë§ˆì´ê·¸ë ˆì´ì…˜ëœ ë ˆì½”ë“œ ì¡°íšŒ ì‹¤íŒ¨:', migratedError);
    return { total: total || 0, migrated: 0 };
  }

  // Get unique work_record_ids
  const uniqueRecordIds = new Set(migratedRecords?.map(r => r.work_record_id) || []);
  
  return {
    total: total || 0,
    migrated: uniqueRecordIds.size,
  };
};

// ë°ì´í„° êµ¬ì¡° ë¶„ì„ì„ ìœ„í•œ ë””ë²„ê¹… í•¨ìˆ˜
export const analyzeWorkRecordData = async (recordId: number) => {
  console.log(`ğŸ” Record ID ${recordId} ë°ì´í„° ë¶„ì„ ì‹œì‘...`);
  const { data, error } = await supabase
    .from('work_records')
    .select('*')
    .eq('id', recordId)
    .single();

  if (error) {
    console.error('  ğŸ”´ ë ˆì½”ë“œ ì¡°íšŒ ì‹¤íŒ¨:', error);
    return;
  }

  if (!data) {
    console.log('  ğŸŸ¡ ë ˆì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    return;
  }

  console.log('  âœ… ë ˆì½”ë“œ ì¡°íšŒ ì„±ê³µ. í•„ë“œ ë¶„ì„:');
  
  for (const key in data) {
    const value = (data as any)[key];
    console.log(`    - í•„ë“œëª…: ${key}`);
    console.log(`      - íƒ€ì…: ${typeof value}`);
    if (typeof value === 'string') {
      console.log(`      - ê¸¸ì´: ${value.length}`);
      console.log(`      - ë‚´ìš© (ì• 100ì): ${value.substring(0, 100)}...`);
      try {
        JSON.parse(value);
        console.log(`      - âœ… JSON íŒŒì‹± ê°€ëŠ¥`);
      } catch (e) {
        console.log(`      - âŒ JSON íŒŒì‹± ë¶ˆê°€ëŠ¥`);
      }
    } else if (value && typeof value === 'object') {
      console.log(`      - ë‚´ìš©: ${JSON.stringify(value, null, 2)}`);
    } else {
      console.log(`      - ê°’: ${value}`);
    }
  }
};

// íŠ¹ì • work_recordì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ ë° ë¶„ì„
export const analyzeSpecificWorkRecord = async (workRecordId: number): Promise<void> => {
  try {
    console.log(`ğŸ” ì‘ì—… ê¸°ë¡ ID ${workRecordId} ìƒì„¸ ë¶„ì„ ì‹œì‘...`)
    
    // 1. ê¸°ë³¸ ì •ë³´ ì¡°íšŒ
    const { data: record, error } = await supabase
      .from('work_records')
      .select('*')
      .eq('id', workRecordId)
      .single()
    
    if (error) {
      console.error(`âŒ ì‘ì—… ê¸°ë¡ ${workRecordId} ì¡°íšŒ ì˜¤ë¥˜:`, error)
      return
    }
    
    if (!record) {
      console.log(`âŒ ì‘ì—… ê¸°ë¡ ${workRecordId}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.`)
      return
    }
    
    console.log(`âœ… ì‘ì—… ê¸°ë¡ ${workRecordId} ê¸°ë³¸ ì •ë³´:`)
    console.log('  - ID:', record.id)
    console.log('  - ê³ ê° ID:', record.customer_id)
    console.log('  - ì¥ë¹„ ID:', record.equipment_id)
    console.log('  - ì‘ì—… ë‚ ì§œ:', record.work_date)
    console.log('  - ì‘ì—… ìœ í˜•:', record.work_type)
    console.log('  - ì´ ê°€ê²©:', record.total_price)
    console.log('  - ìƒíƒœ:', record.status)
    console.log('  - ECU ë©”ì´ì»¤:', record.ecu_maker)
    console.log('  - ECU ëª¨ë¸:', record.ecu_model)
    console.log('  - ACU ì œì¡°ì‚¬:', record.acu_manufacturer)
    console.log('  - ACU ëª¨ë¸:', record.acu_model)
    console.log('  - ì—°ê²° ë°©ë²•:', record.connection_method)
    
    // 2. remapping_works ë¶„ì„
    if (record.remapping_works) {
      console.log('ğŸ“‹ remapping_works ë¶„ì„:')
      console.log('  - íƒ€ì…:', typeof record.remapping_works)
      console.log('  - ì›ë³¸ ë°ì´í„°:', record.remapping_works)
      
      try {
        const parsedWorks = typeof record.remapping_works === 'string' 
          ? JSON.parse(record.remapping_works)
          : record.remapping_works
        
        console.log('  - íŒŒì‹±ëœ ë°ì´í„°:', parsedWorks)
        console.log('  - ë°°ì—´ ì—¬ë¶€:', Array.isArray(parsedWorks))
        
        if (Array.isArray(parsedWorks) && parsedWorks.length > 0) {
          const firstWork = parsedWorks[0]
          console.log('  - ì²« ë²ˆì§¸ ì‘ì—…:', firstWork)
          
          if (firstWork.files) {
            console.log('  - íŒŒì¼ ë°ì´í„° ì¡´ì¬:', !!firstWork.files)
            console.log('  - íŒŒì¼ ë°ì´í„° íƒ€ì…:', typeof firstWork.files)
            console.log('  - íŒŒì¼ ë°ì´í„°:', firstWork.files)
          }
        }
      } catch (parseError) {
        console.error('  - JSON íŒŒì‹± ì˜¤ë¥˜:', parseError)
      }
    } else {
      console.log('âŒ remapping_worksê°€ ì—†ìŠµë‹ˆë‹¤.')
    }
    
    // 3. files í•„ë“œ ë¶„ì„
    if (record.files) {
      console.log('ğŸ“ files í•„ë“œ ë¶„ì„:')
      console.log('  - íƒ€ì…:', typeof record.files)
      console.log('  - ì›ë³¸ ë°ì´í„°:', record.files)
      
      try {
        const parsedFiles = typeof record.files === 'string' 
          ? JSON.parse(record.files)
          : record.files
        
        console.log('  - íŒŒì‹±ëœ íŒŒì¼ ë°ì´í„°:', parsedFiles)
        
        if (Array.isArray(parsedFiles)) {
          console.log(`  - íŒŒì¼ ê°œìˆ˜: ${parsedFiles.length}ê°œ`)
          parsedFiles.forEach((file, index) => {
            console.log(`    íŒŒì¼ ${index + 1}:`, {
              name: file.name,
              size: file.size,
              type: file.type,
              category: file.category,
              hasData: !!file.data,
              dataLength: file.data?.length || 0
            })
          })
        }
      } catch (parseError) {
        console.error('  - íŒŒì¼ JSON íŒŒì‹± ì˜¤ë¥˜:', parseError)
      }
    } else {
      console.log('âŒ files í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.')
    }
    
    // 4. ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒíƒœ í™•ì¸
    const { data: migratedFiles, error: migrationError } = await supabase
      .from('file_metadata')
      .select('*')
      .eq('work_record_id', workRecordId)
    
    if (migrationError) {
      console.error('  - ë§ˆì´ê·¸ë ˆì´ì…˜ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜:', migrationError)
    } else {
      console.log(`ğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒíƒœ: ${migratedFiles?.length || 0}ê°œ íŒŒì¼ì´ Storageì— ì €ì¥ë¨`)
      if (migratedFiles && migratedFiles.length > 0) {
        migratedFiles.forEach((file, index) => {
          console.log(`  ë§ˆì´ê·¸ë ˆì´ì…˜ëœ íŒŒì¼ ${index + 1}:`, {
            fileName: file.file_name,
            category: file.category,
            bucketName: file.bucket_name,
            storageUrl: file.storage_url
          })
        })
      }
    }
    
  } catch (error) {
    console.error(`âŒ ì‘ì—… ê¸°ë¡ ${workRecordId} ë¶„ì„ ì˜¤ë¥˜:`, error)
  }
}

// ë§ˆì´ê·¸ë ˆì´ì…˜ ëŒ€ìƒì¸ ê°€ì¥ ìµœì‹  ì‘ì—… ê¸°ë¡ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
export const analyzeLatestWorkRecordWithFiles = async (): Promise<void> => {
  try {
    console.log('ğŸ”„ ë§ˆì´ê·¸ë ˆì´ì…˜ ëŒ€ìƒ ìµœì‹  ì‘ì—… ê¸°ë¡ ë¶„ì„ ì‹œì‘...');

    const { data: latestRecord, error } = await supabase
      .from('work_records')
      .select('id')
      .not('remapping_works', 'is', null)
      .order('id', { ascending: false })
      .limit(1)
      .single();

    if (error || !latestRecord) {
      console.log('ğŸŸ¡ ë§ˆì´ê·¸ë ˆì´ì…˜í•  íŒŒì¼ì´ í¬í•¨ëœ ì‘ì—… ê¸°ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
      if (error && error.code !== 'PGRST116') { // 'PGRST116' (no rows) is an expected outcome here.
         console.error('ìµœì‹  ê¸°ë¡ ì¡°íšŒ ì˜¤ë¥˜:', error);
      }
      return;
    }

    console.log(`âœ… ìµœì‹  ì‘ì—… ê¸°ë¡ ID ${latestRecord.id}ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ìƒì„¸ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.`);
    await analyzeSpecificWorkRecord(latestRecord.id);

  } catch (err) {
    console.error('ğŸ’¥ ìµœì‹  ì‘ì—… ê¸°ë¡ ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ:', err);
  }
};

// ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ìš”ì•½
export const getDatabaseSummary = async (): Promise<void> => {
  try {
    console.log('ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ìš”ì•½ ì‹œì‘...')
    
    // 1. ì „ì²´ ì‘ì—… ê¸°ë¡ ìˆ˜
    const { count: totalRecords } = await supabase
      .from('work_records')
      .select('*', { count: 'exact', head: true })
    
    // 2. remapping_worksê°€ ìˆëŠ” ê¸°ë¡ ìˆ˜
    const { count: recordsWithRemapping } = await supabase
      .from('work_records')
      .select('*', { count: 'exact', head: true })
      .not('remapping_works', 'is', null)
    
    // 3. filesê°€ ìˆëŠ” ê¸°ë¡ ìˆ˜
    const { count: recordsWithFiles } = await supabase
      .from('work_records')
      .select('*', { count: 'exact', head: true })
      .not('files', 'is', null)
    
    // 4. ë§ˆì´ê·¸ë ˆì´ì…˜ëœ íŒŒì¼ ìˆ˜
    const { count: migratedFiles } = await supabase
      .from('file_metadata')
      .select('*', { count: 'exact', head: true })
    
    // 5. ê³ ìœ í•œ work_record_id ìˆ˜ (ë§ˆì´ê·¸ë ˆì´ì…˜ëœ)
    const { data: uniqueWorkRecords } = await supabase
      .from('file_metadata')
      .select('work_record_id')
    
    const uniqueCount = new Set(uniqueWorkRecords?.map(r => r.work_record_id) || []).size
    
    console.log('ğŸ“ˆ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ìš”ì•½:')
    console.log(`  - ì „ì²´ ì‘ì—… ê¸°ë¡: ${totalRecords || 0}ê°œ`)
    console.log(`  - remapping_works ìˆëŠ” ê¸°ë¡: ${recordsWithRemapping || 0}ê°œ`)
    console.log(`  - files í•„ë“œ ìˆëŠ” ê¸°ë¡: ${recordsWithFiles || 0}ê°œ`)
    console.log(`  - ë§ˆì´ê·¸ë ˆì´ì…˜ëœ íŒŒì¼: ${migratedFiles || 0}ê°œ`)
    console.log(`  - ë§ˆì´ê·¸ë ˆì´ì…˜ëœ ì‘ì—… ê¸°ë¡: ${uniqueCount}ê°œ`)
    console.log(`  - ë§ˆì´ê·¸ë ˆì´ì…˜ ì§„í–‰ë¥ : ${totalRecords ? (uniqueCount / totalRecords * 100).toFixed(1) : 0}%`)
    
    // 6. ìµœê·¼ 5ê°œ ì‘ì—… ê¸°ë¡ ID í‘œì‹œ
    const { data: recentRecords } = await supabase
      .from('work_records')
      .select('id, work_date, ecu_maker, acu_manufacturer')
      .order('created_at', { ascending: false })
      .limit(5)
    
    console.log('ğŸ“‹ ìµœê·¼ ì‘ì—… ê¸°ë¡ 5ê°œ:')
    recentRecords?.forEach((record, index) => {
      console.log(`  ${index + 1}. ID: ${record.id}, ë‚ ì§œ: ${record.work_date}, ECU: ${record.ecu_maker || 'N/A'}, ACU: ${record.acu_manufacturer || 'N/A'}`)
    })
    
  } catch (error) {
    console.error('âŒ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ìš”ì•½ ì˜¤ë¥˜:', error)
  }
} 