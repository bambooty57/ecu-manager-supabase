# 🚀 Supabase 직접 마이그레이션 가이드

## 📋 준비사항
1. Supabase 대시보드 접속: https://supabase.com/dashboard
2. 프로젝트 선택: `ecu-manager-supabase`
3. SQL Editor 접속

## ✨ 0단계: file_metadata 테이블 생성 (가장 먼저 실행!)
이 테이블은 마이그레이션된 파일 정보를 저장하므로, 다른 모든 스크립트보다 먼저 실행해야 합니다.

```sql
-- ==========================================
-- 📄 file_metadata 테이블 생성 스크립트
-- ==========================================
CREATE TABLE IF NOT EXISTS public.file_metadata (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() NOT NULL,
    work_record_id BIGINT REFERENCES public.work_records(id) ON DELETE CASCADE,
    file_name TEXT NOT NULL,
    original_name TEXT,
    file_size BIGINT,
    file_type TEXT,
    category TEXT,
    bucket_name TEXT,
    storage_path TEXT UNIQUE,
    storage_url TEXT UNIQUE,
    is_migrated BOOLEAN DEFAULT TRUE,
    migrated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- 주석 추가
COMMENT ON TABLE public.file_metadata IS 'Storage로 마이그레이션된 파일들의 메타데이터를 저장합니다.';
COMMENT ON COLUMN public.file_metadata.work_record_id IS 'work_records 테이블 외래키';
COMMENT ON COLUMN public.file_metadata.file_name IS 'Storage에 저장된 고유 파일명';
COMMENT ON COLUMN public.file_metadata.original_name IS '사용자가 업로드한 원본 파일명';

-- 인덱스 생성
CREATE INDEX IF NOT EXISTS idx_file_metadata_work_record_id ON public.file_metadata(work_record_id);
CREATE INDEX IF NOT EXISTS idx_file_metadata_file_name ON public.file_metadata(file_name);

-- RLS 활성화 및 정책 설정
ALTER TABLE public.file_metadata ENABLE ROW LEVEL SECURITY;
CREATE POLICY "Public read access for file metadata" ON public.file_metadata FOR SELECT USING (true);
CREATE POLICY "Users can manage their own file metadata" ON public.file_metadata FOR ALL USING (auth.uid() = (SELECT user_id FROM work_records WHERE id = work_record_id)) WITH CHECK (auth.uid() = (SELECT user_id FROM work_records WHERE id = work_record_id));

SELECT '✅ file_metadata 테이블 생성 및 RLS 설정 완료!';
```

## 🔍 1단계: 데이터 상태 분석

### 1-1. 기본 상태 확인
```sql
-- 전체 데이터 상태 확인
SELECT 
  '전체 작업 기록 수' as 구분,
  COUNT(*) as 개수
FROM work_records

UNION ALL

SELECT 
  'remapping_works 있는 기록',
  COUNT(*)
FROM work_records 
WHERE remapping_works IS NOT NULL

UNION ALL

SELECT 
  'files 필드 있는 기록',
  COUNT(*)
FROM work_records 
WHERE files IS NOT NULL

UNION ALL

SELECT 
  '마이그레이션된 파일',
  COUNT(*)
FROM file_metadata;
```

### 1-2. 상세 작업 기록 분석
```sql
-- 최근 10개 작업 기록 상태
SELECT 
  id,
  work_date,
  ecu_maker,
  acu_manufacturer,
  CASE 
    WHEN remapping_works IS NOT NULL THEN '있음'
    ELSE '없음'
  END as remapping_works_상태,
  CASE 
    WHEN files IS NOT NULL THEN '있음'
    ELSE '없음'
  END as files_상태,
  created_at
FROM work_records 
ORDER BY created_at DESC 
LIMIT 10;
```

### 1-3. 파일 데이터 구조 확인
```sql
-- 실제 파일 데이터 추출 확인
WITH file_extraction AS (
  SELECT 
    wr.id as work_record_id,
    wr.work_date,
    wr.remapping_works,
    -- JSON에서 파일 데이터 추출 시도
    CASE 
      WHEN jsonb_typeof(wr.remapping_works) = 'array' THEN
        wr.remapping_works->0->'files'
      ELSE
        wr.remapping_works->'files'
    END as extracted_files
  FROM work_records wr
  WHERE wr.remapping_works IS NOT NULL
    AND wr.remapping_works::text != 'null'
    AND wr.remapping_works::text != '[]'
)
SELECT 
  work_record_id,
  work_date,
  CASE 
    WHEN extracted_files IS NOT NULL THEN '파일 데이터 있음'
    ELSE '파일 데이터 없음'
  END as 파일_상태,
  jsonb_typeof(extracted_files) as 파일_타입
FROM file_extraction
ORDER BY work_record_id DESC
LIMIT 10;
```

## 🔧 2단계: Storage 버킷 설정

### 2-1. 버킷 생성
```sql
-- Storage 버킷 생성
INSERT INTO storage.buckets (id, name, public, file_size_limit, allowed_mime_types)
VALUES 
  ('work-files', 'work-files', true, 52428800, ARRAY['application/pdf', 'text/plain', 'application/octet-stream']),
  ('work-media', 'work-media', true, 104857600, ARRAY['image/jpeg', 'image/png', 'image/gif', 'image/webp', 'video/mp4', 'video/avi'])
ON CONFLICT (id) DO NOTHING;
```

### 2-2. Storage 정책 설정
```sql
-- 읽기 정책
INSERT INTO storage.policies (id, bucket_id, name, definition, check_expression, command)
VALUES 
  ('work-files-read', 'work-files', 'Anyone can view work files', 'true', 'true', 'SELECT'),
  ('work-media-read', 'work-media', 'Anyone can view work media', 'true', 'true', 'SELECT')
ON CONFLICT (id) DO NOTHING;

-- 쓰기 정책
INSERT INTO storage.policies (id, bucket_id, name, definition, check_expression, command)
VALUES 
  ('work-files-write', 'work-files', 'Authenticated users can upload work files', 'auth.role() = ''authenticated''', 'auth.role() = ''authenticated''', 'INSERT'),
  ('work-media-write', 'work-media', 'Authenticated users can upload work media', 'auth.role() = ''authenticated''', 'auth.role() = ''authenticated''', 'INSERT')
ON CONFLICT (id) DO NOTHING;
```

## 🚀 3단계: 마이그레이션 실행

### 3-1. 마이그레이션 함수 생성
```sql
-- 파일 메타데이터 생성 함수
CREATE OR REPLACE FUNCTION create_file_metadata_only()
RETURNS TABLE(
  work_record_id INTEGER,
  processed_files INTEGER,
  status TEXT
) AS $$
DECLARE
  record_row RECORD;
  file_data JSONB;
  file_item JSONB;
  file_counter INTEGER;
  unique_filename TEXT;
  bucket_name TEXT;
  storage_path TEXT;
  base64_data TEXT;
  file_size INTEGER;
BEGIN
  -- 마이그레이션 대상 기록들을 순회
  FOR record_row IN 
    SELECT id, remapping_works, files
    FROM work_records 
    WHERE remapping_works IS NOT NULL 
      AND remapping_works::text != 'null'
      AND remapping_works::text != '[]'
      AND LENGTH(remapping_works::text) > 10
    ORDER BY id
  LOOP
    file_counter := 0;
    
    -- remapping_works에서 파일 데이터 추출
    BEGIN
      -- 배열인 경우 첫 번째 요소에서 files 추출
      IF jsonb_typeof(record_row.remapping_works) = 'array' THEN
        file_data := record_row.remapping_works->0->'files';
      ELSE
        file_data := record_row.remapping_works->'files';
      END IF;
      
      -- files 필드에서도 확인
      IF file_data IS NULL AND record_row.files IS NOT NULL THEN
        file_data := record_row.files;
      END IF;
      
      -- 파일 데이터가 있는 경우 처리
      IF file_data IS NOT NULL AND jsonb_typeof(file_data) = 'array' THEN
        -- 각 파일을 순회하며 file_metadata에 기록
        FOR file_item IN SELECT * FROM jsonb_array_elements(file_data)
        LOOP
          -- 파일에 필요한 정보가 있는지 확인
          IF file_item ? 'name' AND file_item ? 'data' AND LENGTH(file_item->>'data') > 100 THEN
            file_counter := file_counter + 1;
            
            -- 고유 파일명 생성
            unique_filename := EXTRACT(EPOCH FROM NOW())::TEXT || '_' || 
                             substr(md5(random()::text), 1, 8) || '_' || 
                             (file_item->>'name');
            
            -- 버킷 결정
            CASE 
              WHEN LOWER(file_item->>'name') ~ '\.(jpg|jpeg|png|gif|webp|avif)$' THEN
                bucket_name := 'work-media';
              WHEN LOWER(file_item->>'name') ~ '\.(mp4|avi|mov|wmv|flv)$' THEN
                bucket_name := 'work-media';
              ELSE
                bucket_name := 'work-files';
            END CASE;
            
            -- Storage 경로 생성
            storage_path := record_row.id::TEXT || '/' || unique_filename;
            
            -- Base64 데이터에서 파일 크기 추정
            base64_data := file_item->>'data';
            IF base64_data LIKE 'data:%' THEN
              base64_data := substring(base64_data from position(',' in base64_data) + 1);
            END IF;
            file_size := LENGTH(base64_data) * 3 / 4; -- Base64 크기 추정
            
            -- file_metadata 테이블에 기록 (중복 체크)
            INSERT INTO file_metadata (
              work_record_id,
              file_name,
              original_name,
              file_size,
              file_type,
              category,
              bucket_name,
              storage_path,
              storage_url,
              created_at
            ) 
            SELECT 
              record_row.id,
              unique_filename,
              file_item->>'name',
              file_size,
              COALESCE(file_item->>'type', 'application/octet-stream'),
              COALESCE(file_item->>'category', 'unknown'),
              bucket_name,
              storage_path,
              'https://ewxzampbdpuaawzrvsln.supabase.co/storage/v1/object/public/' || bucket_name || '/' || storage_path,
              NOW()
            WHERE NOT EXISTS (
              SELECT 1 FROM file_metadata 
              WHERE work_record_id = record_row.id 
                AND original_name = file_item->>'name'
            );
          END IF;
        END LOOP;
      END IF;
      
    EXCEPTION WHEN OTHERS THEN
      -- JSON 파싱 오류 등 무시하고 계속 진행
      CONTINUE;
    END;
    
    -- 결과 반환
    work_record_id := record_row.id;
    processed_files := file_counter;
    status := CASE 
      WHEN file_counter > 0 THEN 'METADATA_CREATED'
      ELSE 'NO_FILES'
    END;
    
    RETURN NEXT;
  END LOOP;
  
  RETURN;
END;
$$ LANGUAGE plpgsql;
```

### 3-2. 마이그레이션 실행
```sql
-- 마이그레이션 실행
SELECT * FROM create_file_metadata_only();
```

## 📊 4단계: 결과 확인

### 4-1. 마이그레이션 결과 확인
```sql
-- 마이그레이션된 파일 목록
SELECT 
  fm.work_record_id,
  wr.work_date,
  wr.ecu_maker,
  COUNT(fm.id) as 마이그레이션된_파일수,
  STRING_AGG(fm.file_name, ', ') as 파일목록
FROM file_metadata fm
JOIN work_records wr ON fm.work_record_id = wr.id
GROUP BY fm.work_record_id, wr.work_date, wr.ecu_maker
ORDER BY fm.work_record_id DESC;
```

### 4-2. Storage 버킷별 파일 수 확인
```sql
-- 버킷별 파일 통계
SELECT 
  bucket_name,
  COUNT(*) as 파일수,
  COUNT(DISTINCT work_record_id) as 작업기록수
FROM file_metadata
GROUP BY bucket_name;
```

### 4-3. 마이그레이션 통계
```sql
-- 전체 마이그레이션 통계
SELECT 
  '마이그레이션 완료' as 상태,
  COUNT(DISTINCT work_record_id) as 완료된_작업기록,
  COUNT(*) as 총_마이그레이션된_파일,
  ROUND(AVG(file_size)::NUMERIC, 2) as 평균_파일크기_bytes
FROM file_metadata;
```

## 🔧 5단계: 문제 해결

### 5-1. 중복 데이터 제거
```sql
-- 중복된 file_metadata 제거
DELETE FROM file_metadata 
WHERE id NOT IN (
  SELECT MIN(id) 
  FROM file_metadata 
  GROUP BY work_record_id, original_name
);
```

### 5-2. 마이그레이션 재실행 (필요시)
```sql
-- 기존 메타데이터 삭제 후 재실행
DELETE FROM file_metadata;
SELECT * FROM create_file_metadata_only();
```

## ⚠️ 주의사항

1. **백업 필수**: 마이그레이션 전에 데이터베이스 백업을 수행하세요.
2. **단계별 실행**: 각 단계를 순서대로 실행하고 결과를 확인하세요.
3. **데이터 검증**: 마이그레이션 후 데이터 무결성을 확인하세요.
4. **성능 모니터링**: 대량 데이터 처리 시 성능을 모니터링하세요.

## 🎯 예상 결과

- ✅ `file_metadata` 테이블에 파일 정보 저장
- ✅ Storage URL 생성 (실제 파일은 별도 업로드 필요)
- ✅ 파일 타입별 버킷 분류
- ✅ 중복 방지 및 오류 처리

## 📞 문제 발생 시

1. 콘솔에서 오류 메시지 확인
2. 데이터 구조 분석 쿼리로 원인 파악
3. 단계별로 다시 실행
4. 필요시 개발팀에 문의 